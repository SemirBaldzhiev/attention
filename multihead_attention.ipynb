{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semir/.local/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Decoder Attention Explained\n",
    "\n",
    "- As we saw in the previous notebooks we have Encoder-only Transformer and Decoder-Only Transformer. </br>\n",
    "Before that the first transformer ever made had one part called Encoder that used self attention, and one part called Decoder that used Masked Self-Attention.</br>\n",
    "This 2 parts were connected together to each other so they could calculate something called Encoder-Decoder Attention. </br>\n",
    "Encoder-Decoder Attention uses the output from the Encoder to calculate the Keys, Values and Queries are calculated from the output of the Masked Self-Attention </br>\n",
    "(Decoder). Once the Q, K and V are calculated the Encoder-Decoder Attention is calculated just like Self-Attention using every similarity.</br>\n",
    "This first Transformer is based on something called Seq2Seq or an Encoder-Decoder model. Seq2Seq were designed to translate text from one language into another.</br>\n",
    "Encoder-Decoder Attention is also called Cross-Attention.\n",
    "\n",
    "- We can apply attention to the encoded values multiple times simultaneously, if we want to work with longer sequences and to understand how the word are related in this long sequences of words/tokens. Each attention is called head and has its own sets of weights for calculating the Q, K and V. </br>\n",
    "When we have multiple heads we call it Multi-Head Attention.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Implementation of Attention class which can work as a Self-Attention and Mask Self-Attention\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_model=2, row_dim_idx=0, col_dim_idx=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim_idx = row_dim_idx\n",
    "        self.col_dim_idx = col_dim_idx\n",
    "        \n",
    "    \n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v ,mask=None):\n",
    "        \n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "        \n",
    "        sims = torch.matmul(q, k.transpose(self.row_dim_idx, self.col_dim_idx))\n",
    "        \n",
    "        sims_scaled = sims / torch.tensor(k.size(self.col_dim_idx) ** 0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            sims_scaled = sims_scaled.masked_fill(mask, values=-1.e9)\n",
    "        \n",
    "        \n",
    "        attention_percents = F.softmax(sims_scaled, dim=self.col_dim_idx)\n",
    "        \n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        \n",
    "        return attention_scores\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create matrices of token encodings...\n",
    "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "# set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create an attention object\n",
    "attention = Attention(d_model=2,\n",
    "                      row_dim_idx=0,\n",
    "                      col_dim_idx=1)\n",
    "\n",
    "# calculate encoder-decoder attention\n",
    "attention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    \"Implementation of MultiHead Attention which can execue and calculate more than one set of Weights for Q, K, V\n",
    "     We can specify the number of attention heads from the num_heads argument wich by default is 1\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_model=2, row_dim_idx=0, col_dim_idx=1, num_heads=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList([Attention(d_model, row_dim_idx, col_dim_idx) for _ in range(num_heads)])\n",
    "        self.row_dim_idx = row_dim_idx\n",
    "        self.col_dim_idx = col_dim_idx\n",
    "    \n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodingds_for_v):\n",
    "        return torch.cat(\n",
    "                        [head(encodings_for_q, encodings_for_k, encodingds_for_v) \n",
    "                        for head in self.heads], \n",
    "                        dim=self.col_dim_idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MultiHead Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim_idx=0,\n",
    "                                        col_dim_idx=1,\n",
    "                                        num_heads=1)\n",
    "\n",
    "# calculate encoder-decoder attention\n",
    "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0100,  1.0641, -0.7081, -0.8268],\n",
       "        [ 0.2040,  0.7057, -0.7417, -0.9193],\n",
       "        [ 3.4989,  2.2427, -0.7190, -0.8447]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim_idx=0,\n",
    "                                        col_dim_idx=1,\n",
    "                                        num_heads=2)\n",
    "\n",
    "# calculate encoder-decoder attention\n",
    "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
