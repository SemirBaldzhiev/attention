{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semir/.local/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # access the softmax function for calculating the attention weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention class explanation\n",
    "- nn.Module is a base class for all nural network modules that you make with PyTorch\n",
    "- SelfAttention inherite this base class\n",
    "- d_model is a dimmesion of the model or in other words the Word Embedding values per token.\n",
    "We use it to define the dimesion of the weight matrices for Queries, Keys and Values. In this case they are going to be with dimesion 2x2\n",
    "\n",
    "- token_encodings are Word Embeddings + Positional Encoding for each input token\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    '''\n",
    "    This class implements the basic self-attention mechanism.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_model=2, row_dim=0, col_dim=1):\n",
    "        super().__init__() # call the __init__ method of the parent class\n",
    "        \n",
    "        # Query weights matrix which hold and calculate the query values\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        # Key weights matrix which hold and calculate the key values\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        # Value weights matrix which hold and calculate the value values\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "    \n",
    "    def forward(self, token_encodings):\n",
    "        \n",
    "        q = self.W_q(token_encodings)\n",
    "        k = self.W_k(token_encodings)\n",
    "        v = self.W_v(token_encodings)\n",
    "        \n",
    "        # For test purposes\n",
    "        # print('Q = ', q)\n",
    "        # print('K = ', k)\n",
    "        # print('V = ', v)\n",
    "        \n",
    "        # Calculate similarity scores q * k^T\n",
    "        sims = torch.matmul(q, k.transpose(self.row_dim, self.col_dim))\n",
    "        \n",
    "        # Calculate scaled similarity scores\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim) ** 0.5)\n",
    "        \n",
    "        # Apply softmax to determine what percent of each tokens value to\n",
    "        # use in the final attention values.\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        \n",
    "        # Scale the values by their associated percentages and add them up.\n",
    "        attension_scores = torch.matmul(attention_percents, v)\n",
    "        \n",
    "        return attension_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a matrix of token encodings...\n",
    "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                 [0.57, 1.36],\n",
    "                                 [4.41, -2.16]])\n",
    "\n",
    "# set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create a basic self-attention ojbect\n",
    "selfAttention = SelfAttention(d_model=2,\n",
    "                               row_dim=0,\n",
    "                               col_dim=1)\n",
    "\n",
    "# calculate basic attention for the token encodings\n",
    "selfAttention(encodings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify calculations and print intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5406, -0.1657],\n",
      "        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[-0.1549, -0.3443],\n",
      "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[ 0.6233,  0.6146],\n",
      "        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## print out the weight matrix that creates the queries, keys and values\n",
    "print(selfAttention.W_q.weight.transpose(0, 1))\n",
    "print(selfAttention.W_k.weight.transpose(0, 1))\n",
    "print(selfAttention.W_v.weight.transpose(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7621, -0.0428],\n",
      "        [ 1.1063,  0.7890],\n",
      "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.1469, -0.3038],\n",
      "        [ 0.1057,  0.3685],\n",
      "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 0.6038,  0.7434],\n",
      "        [-0.3502,  0.5303],\n",
      "        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(selfAttention.W_q(encodings_matrix))\n",
    "print(selfAttention.W_k(encodings_matrix))\n",
    "print(selfAttention.W_v(encodings_matrix))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7621, -0.0428],\n",
       "        [ 1.1063,  0.7890],\n",
       "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = selfAttention.W_q(encodings_matrix)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1469, -0.3038],\n",
       "        [ 0.1057,  0.3685],\n",
       "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = selfAttention.W_k(encodings_matrix)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0990,  0.0648, -0.6523],\n",
       "        [-0.4022,  0.4078, -3.0024],\n",
       "        [ 0.4842, -0.6683,  4.0461]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = torch.matmul(q, k.transpose(dim0=0, dim1=1))\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0700,  0.0458, -0.4612],\n",
       "        [-0.2844,  0.2883, -2.1230],\n",
       "        [ 0.3424, -0.4725,  2.8610]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_sims = sims / (torch.tensor(2)**0.5)\n",
    "scaled_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3573, 0.4011, 0.2416],\n",
       "        [0.3410, 0.6047, 0.0542],\n",
       "        [0.0722, 0.0320, 0.8959]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_percents = F.softmax(scaled_sims, dim=1)\n",
    "attention_percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = torch.matmul(attention_percents, selfAttention.W_v(encodings_matrix))\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
