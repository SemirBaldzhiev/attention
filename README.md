# Attention in Transformers: Concepts and Code in PyTorch

This repository contains exercises from the course **"Attention in Transformers: Concepts and Code in PyTorch"** by DeepLearning.AI, taught by **Josh Starmer**.

## Overview
This course explores the core concepts behind attention mechanisms in Transformers, implementing them in **PyTorch**. The exercises in this repository will help reinforce the theoretical and practical understanding of attention in deep learning models.

## Contents
- Implementation of **self-attention**
- Understanding **scaled dot-product attention**
- Exploring **multi-head attention**
- Building Transformer blocks from scratch
- Practical coding exercises in **PyTorch**

## Installation
To run the exercises, clone this repository and install the required dependencies:

```sh
git clone https://github.com/SemirBaldzhiev/attention
cd attention
pip install -r requirements.txt
```

## Usage
You can run the exercises in a Jupyter Notebook or in VS Code with Jupyter notebook extension


## References
- [DeepLearning.AI Course](https://www.deeplearning.ai/)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)

---
Happy coding! ðŸš€


